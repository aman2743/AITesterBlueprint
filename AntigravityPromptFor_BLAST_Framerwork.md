**Protocol 0**: **Initialization (Mandatory)**

**Prompt 1:** 
@BLAST.md Read from the file and lets start the way, We are going to create a local LLM
Testcase generator with the ollama. I will be share the prompt for the
testcase generator.

**Phase 1**: **B - Blueprint (Vision & Logic)**

**Prompt 1:** 
@BLAST.md lets startwith first Phase 1:Blueprint, lets try to create local test generator with Ollama API

**Prompt 2:**
**North Star**:Local LLM test cases generator based on user input with a proper template which we are going to store in our code with
Ollama API (open source model llama 3.2)
**Integrations**:Ollama
**Source Of Truth**: NA
**Delivery Payload**: A UI Chat where user will enter the input and based on the input Ollama will take the input and generate the test cases for us
**Behaviour Rules**: User enter a input based on the input we will give the output using Local LLM in Ollama

**Phase 2**: **L - Link (Connectivity)**
@BLAST.md Please execute Phase 2

**Create Read Me File**
Please add README file with proper diagram of how this project works
